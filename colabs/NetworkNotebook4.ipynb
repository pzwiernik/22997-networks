{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Random Networks Beyond Erdős–Rényi\n",
        "\n",
        "This Colab walks through several random network models and community-detection ideas **beyond** the basic $G(n,p)$.  \n",
        "It follows the outline from the lecture:\n",
        "- Configuration model (random wiring with a given degree sequence)  \n",
        "- p2 (degree-heterogeneity via node effects)  \n",
        "- Latent-space random graphs  \n",
        "- Preferential attachment (Barabási–Albert)  \n",
        "- Community detection: modularity & Girvan–Newman  \n",
        "- Stochastic Block Model (SBM) + spectral clustering\n",
        "\n",
        "**Dependencies:** `networkx`, `numpy`, `scipy`, `scikit-learn`, `matplotlib`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# --- Setup\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.sparse.linalg import eigsh\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "rng = np.random.default_rng(7)\n",
        "\n",
        "plt.rcParams.update({\n",
        "    \"figure.figsize\": (6, 4),\n",
        "    \"axes.grid\": True\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 0) Quick baseline: Erdős–Rényi $G(n,p)$\n",
        "\n",
        "Edges are independent with probability $p$. Degree concentration is tight, clustering is low, and there is no community structure by design.\n",
        "We'll use it only as a reference point.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# Small ER demo\n",
        "n, p = 200, 0.025\n",
        "G_er = nx.gnp_random_graph(n, p, seed=7)\n",
        "deg_er = np.array([d for _, d in G_er.degree()])\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.hist(deg_er, bins=20)\n",
        "ax.set_xlabel(\"degree\")\n",
        "ax.set_ylabel(\"count\")\n",
        "ax.set_title(\"G(n,p): degree histogram\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 1) Configuration Model\n",
        "\n",
        "**Goal.** Generate a random network with a *given degree sequence* $\\{k_1,\\dots,k_n\\}$.  \n",
        "**Construction.** Give node $i$ exactly $k_i$ stubs, then **uniformly pair** all $2L=\\sum_i k_i$ stubs (perfect matching).  \n",
        "This samples uniformly among all multigraphs with that degree sequence.\n",
        "\n",
        "**Expected adjacency.** For nodes $i\\neq j$ with degrees $k_i,k_j$ and $2L=\\sum_i k_i$,\n",
        "\\begin{equation}\n",
        "\\mathbb{E}[A_{ij}] \\;=\\; \\frac{k_i k_j}{2L-1} \\;\\approx\\; \\frac{k_i k_j}{2L}.\n",
        "\\end{equation}\n",
        "\n",
        "> This is an **expected edge count** under random stub pairing (a multigraph baseline). It is not a probability and can exceed 1 when both degrees are large.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# Helper: sample configuration-model simple graph (collapse multi-edges, remove loops)\n",
        "def configuration_simple_graph(deg_seq, seed=None):\n",
        "    M = nx.configuration_model(deg_seq, seed=seed)\n",
        "    G = nx.Graph(M)\n",
        "    G.remove_edges_from(nx.selfloop_edges(G))\n",
        "    return nx.Graph(G)\n",
        "\n",
        "# Example degree sequence: heterogeneous\n",
        "n = 120\n",
        "deg_seq = rng.integers(1, 8, size=n).tolist()  # degrees 1..7\n",
        "if sum(deg_seq) % 2 == 1:\n",
        "    deg_seq[0] += 1\n",
        "\n",
        "G_conf = configuration_simple_graph(deg_seq, seed=7)\n",
        "\n",
        "# Compare empirical E[A_ij] to baseline ki*kj/(2L-1) on a small subset of pairs\n",
        "pairs = [(int(rng.integers(0, n)), int(rng.integers(0, n))) for _ in range(300)]\n",
        "pairs = [(i, j) for i, j in pairs if i < j][:60]\n",
        "\n",
        "def monte_carlo_mean_Aij(pairs, deg_seq, trials=200, seed=7):\n",
        "    rng_local = np.random.default_rng(seed)\n",
        "    counts = {pair: 0 for pair in pairs}\n",
        "    for t in range(trials):\n",
        "        Gt = configuration_simple_graph(deg_seq, seed=int(rng_local.integers(0, 1<<31)))\n",
        "        for i, j in pairs:\n",
        "            counts[(i, j)] += 1 if Gt.has_edge(i, j) else 0\n",
        "    return {pair: counts[pair] / trials for pair in pairs}\n",
        "\n",
        "mc = monte_carlo_mean_Aij(pairs, deg_seq, trials=200, seed=11)\n",
        "L = sum(deg_seq) // 2\n",
        "ki = np.array(deg_seq)\n",
        "exp_baseline = {pair: (ki[pair[0]] * ki[pair[1]]) / (2*L - 1) for pair in pairs}\n",
        "\n",
        "x = np.array([exp_baseline[p] for p in pairs])\n",
        "y = np.array([mc[p] for p in pairs])\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(x, y, s=20)\n",
        "ax.plot([x.min(), x.max()], [x.min(), x.max()])\n",
        "ax.set_xlabel(\"Expected A_ij ≈ k_i k_j / (2L-1)\")\n",
        "ax.set_ylabel(\"Monte Carlo mean of A_ij (empirical)\")\n",
        "ax.set_title(\"Configuration model: expected vs empirical adjacency\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 2) The p2 Model (degree heterogeneity via node effects)\n",
        "\n",
        "Edges are independent given node effects:\n",
        "\\begin{equation}\n",
        "\\Pr(Y_{ij}=1 \\mid \\alpha_i,\\alpha_j) \\;=\\; \\sigma\\!\\big(\\mu + \\alpha_i + \\alpha_j\\big),\\quad \\sigma(x)=\\frac{1}{1+e^{-x}}.\n",
        "\\end{equation}\n",
        "Here $\\mu$ controls global density and $\\alpha_i$ are node-specific \"sociability\" terms (often zero-mean).  \n",
        "This reduces to $G(n,p)$ if all $\\alpha_i=0$ and induces degree heterogeneity when they vary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "def simulate_p2(n=150, mu=-2.4, sigma_alpha=0.9, seed=7):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    alpha = rng.normal(0.0, sigma_alpha, size=n)\n",
        "    def sigmoid(x): return 1.0 / (1.0 + np.exp(-x))\n",
        "    A = np.zeros((n, n), dtype=int)\n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):\n",
        "            p = sigmoid(mu + alpha[i] + alpha[j])\n",
        "            if rng.random() < p:\n",
        "                A[i, j] = A[j, i] = 1\n",
        "    G = nx.from_numpy_array(A)\n",
        "    return G, alpha\n",
        "\n",
        "G_p2, alpha = simulate_p2()\n",
        "deg_p2 = np.array([d for _, d in G_p2.degree()])\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.hist(deg_p2, bins=20)\n",
        "ax.set_xlabel(\"degree\")\n",
        "ax.set_ylabel(\"count\")\n",
        "ax.set_title(\"p2 model: degree heterogeneity from node effects\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 3) Latent-Space Random Graphs\n",
        "\n",
        "Each node $i$ has a position $z_i \\in \\mathbb{R}^d$ and\n",
        "\\begin{equation}\n",
        "\\Pr(i \\sim j) = f(\\lVert z_i - z_j\\rVert), \\quad f \\text{ decreasing.}\n",
        "\\end{equation}\n",
        "This yields clustering (nearby nodes connect) and occasional long links if we allow a flatter tail.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "def simulate_latent_space(n=200, d=2, beta0=0.0, beta1=3.0, seed=7):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    Z = rng.normal(0.0, 1.0, size=(n, d))\n",
        "    def sigmoid(x): return 1.0 / (1.0 + np.exp(-x))\n",
        "    A = np.zeros((n, n), dtype=int)\n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):\n",
        "            dist = np.linalg.norm(Z[i] - Z[j])\n",
        "            p = sigmoid(beta0 - beta1 * dist)\n",
        "            if rng.random() < p:\n",
        "                A[i, j] = A[j, i] = 1\n",
        "    return nx.from_numpy_array(A), Z\n",
        "\n",
        "G_latent, Z = simulate_latent_space()\n",
        "\n",
        "deg_latent = np.array([d for _, d in G_latent.degree()])\n",
        "fig, ax = plt.subplots()\n",
        "ax.hist(deg_latent, bins=20)\n",
        "ax.set_xlabel(\"degree\")\n",
        "ax.set_ylabel(\"count\")\n",
        "ax.set_title(\"Latent-space model: degree distribution\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 4) Preferential Attachment (Barabási–Albert)\n",
        "\n",
        "A growing model where each new node attaches to existing nodes with probability proportional to their current degree.  \n",
        "This produces heavy-tailed degree distributions with power-law tails (exponent $\\gamma \\approx 3$ in the BA model).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "n, m = 3000, 3\n",
        "G_ba = nx.barabasi_albert_graph(n, m, seed=7)\n",
        "deg_ba = np.array([d for _, d in G_ba.degree()])\n",
        "\n",
        "# CCDF on log-log\n",
        "vals = np.sort(deg_ba)\n",
        "u, counts = np.unique(vals, return_counts=True)\n",
        "pmf = counts / counts.sum()\n",
        "ccdf = 1.0 - np.cumsum(pmf) + pmf  # P(D >= k)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.loglog(u, ccdf, marker=\"o\", linestyle=\"\")\n",
        "ax.set_xlabel(\"k (degree)\")\n",
        "ax.set_ylabel(\"P(D ≥ k)\")\n",
        "ax.set_title(\"Barabási–Albert: degree CCDF (log–log)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 5) Communities: Modularity and Girvan–Newman\n",
        "\n",
        "**Modularity (edge-level form).**\n",
        "\\begin{equation}\n",
        "M \\;=\\; \\frac{1}{2L}\\sum_{i,j}\\left(A_{ij}-\\frac{k_i k_j}{2L}\\right)\\,\\delta(c_i,c_j).\n",
        "\\end{equation}\n",
        "This compares **observed** within-community edges to **expected** edges under the configuration-model baseline.\n",
        "\n",
        "**Girvan–Newman (divisive):** iteratively remove the edge with largest betweenness; track the partition with the best modularity.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# Zachary's Karate Club\n",
        "G_karate = nx.karate_club_graph()\n",
        "\n",
        "from networkx.algorithms.community import girvan_newman\n",
        "from networkx.algorithms.community.quality import modularity\n",
        "\n",
        "comp_levels = []\n",
        "max_levels = 6\n",
        "gen = girvan_newman(G_karate)\n",
        "for level in range(max_levels):\n",
        "    communities = tuple(sorted(c) for c in next(gen))\n",
        "    comp_levels.append(communities)\n",
        "\n",
        "mods = []\n",
        "for comm in comp_levels:\n",
        "    comm_sets = [set(c) for c in comm]\n",
        "    mods.append(modularity(G_karate, comm_sets))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(range(1, len(mods)+1), mods, marker=\"o\")\n",
        "ax.set_xlabel(\"split level\")\n",
        "ax.set_ylabel(\"modularity\")\n",
        "ax.set_title(\"Girvan–Newman on Karate Club: modularity by split level\")\n",
        "plt.show()\n",
        "\n",
        "best_idx = int(np.argmax(mods))\n",
        "print(f\"Best split level (among the first {len(mods)}): {best_idx+1}, modularity={mods[best_idx]:.3f}\")\n",
        "print(\"Community sizes:\", [len(s) for s in comp_levels[best_idx]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 6) Stochastic Block Model (SBM) + Spectral Clustering\n",
        "\n",
        "The SBM plants $K$ communities with different within/between connection probabilities.\n",
        "Spectral methods (eigenvectors of adjacency or Laplacian) reveal the blocks when separation is sufficient.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# Two-block SBM + spectral clustering\n",
        "N1, N2 = 80, 70\n",
        "sizes = [N1, N2]\n",
        "pin, pout = 0.12, 0.02\n",
        "P = [[pin, pout], [pout, pin]]\n",
        "\n",
        "G_sbm = nx.stochastic_block_model(sizes, P, seed=7)\n",
        "A = nx.to_scipy_sparse_array(G_sbm, format=\"csr\", dtype=float)\n",
        "L = nx.normalized_laplacian_matrix(G_sbm)\n",
        "\n",
        "K = 2\n",
        "vals, vecs = eigsh(L, k=K, which=\"SM\")\n",
        "\n",
        "X = vecs / (np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-12)\n",
        "labels = KMeans(n_clusters=2, n_init=20, random_state=7).fit_predict(X)\n",
        "\n",
        "true = np.array([0]*N1 + [1]*N2)\n",
        "acc = max((labels == true).mean(), (1-labels == true).mean())\n",
        "print(f\"Accuracy vs planted partition: {acc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Wrap-up\n",
        "\n",
        "- **Configuration model** is the random-wiring baseline given degrees; $\\mathbb{E}[A_{ij}] \\approx k_i k_j/(2L)$ explains why high-degree pairs connect more even without structure.  \n",
        "- **p2** adds degree heterogeneity via node effects while retaining edge independence.  \n",
        "- **Latent-space** graphs produce clustering from distances in an unobserved space.  \n",
        "- **Preferential attachment** explains heavy-tailed degree distributions as a growth mechanism.  \n",
        "- **Girvan–Newman + modularity** remove bridges and score partitions against the configuration-model baseline.  \n",
        "- **SBM + spectral** recovers planted communities via eigenvector geometry.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}